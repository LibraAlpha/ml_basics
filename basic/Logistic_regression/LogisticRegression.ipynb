{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from basic.mnist import Mnist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# load data from Mnist Dataset\n",
    "dataloader = Mnist()\n",
    "train_data, train_label = dataloader.get_data(train=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(60000, 784)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_data).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss function of Logistic Regression:\n",
    "$$\n",
    "\\scalebox{1.5}{\n",
    "\\begin{align*}\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log\\left(h_\\theta(x^{(i)})\\right) + (1 - y^{(i)}) \\log\\left(1 - h_\\theta(x^{(i)})\\right) \\right]\n",
    "\\end{align*}\n",
    "}\n",
    "$$\n",
    "\n",
    "### Gradient Calculation:\n",
    "$$\\scalebox{1.5} {\n",
    "\\begin{align*}\n",
    "g_i = \\frac{\\partial J(\\theta)} {\\partial w_i}=(p(x_i) - y_i)x_i\n",
    "\\end{align*}\n",
    "}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### how to calculate p(x_i)\n",
    "$$\\scalebox{1.5} {\n",
    "\\begin{align*}\n",
    "p(x_i) = \\frac{1}{1 + e^{-(w^Tx + b)}}\n",
    "\\end{align*}\n",
    "}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def logistic_regression(epochs=10, step = 0.001):\n",
    "    \"\"\"\n",
    "    :param epochs: 迭代次数\n",
    "    :param step: 步长\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for i in range(len(train_data)):\n",
    "        train_data[i].append(1)\n",
    "\n",
    "    train_data_arr = np.array(train_data)\n",
    "\n",
    "    # 初始化w,\n",
    "    w = np.zeros(train_data_arr.shape[1])\n",
    "\n",
    "    # 迭代轮数\n",
    "    for epoch in range(epochs):\n",
    "        # 遍历数据进行调参\n",
    "        if epoch % 10 == 0:\n",
    "            print(epoch)\n",
    "        for idx in range(train_data_arr.shape[0]):\n",
    "            x= train_data_arr[idx]\n",
    "            y = train_label[idx]\n",
    "            wx = np.dot(w, x)\n",
    "            # 梯度下降\n",
    "            px = 1 / (1 + np.exp(-wx))\n",
    "            gradient = (px - y) * px\n",
    "            w += step * gradient\n",
    "\n",
    "    return w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def pred(w, x):\n",
    "    wx = np.dot(w, x)\n",
    "    p = np.exp(wx) / (1 + np.exp(wx))\n",
    "    if p > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def test(w):\n",
    "    test_data, test_label = dataloader.get_data(False)\n",
    "\n",
    "    err_cnt = 0\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        test_data[i].append(1)\n",
    "\n",
    "    for idx in range(len(test_data)):\n",
    "        if test_label[idx] != pred(w, test_data[idx]):\n",
    "            err_cnt += 1\n",
    "\n",
    "    return round(1 - err_cnt / len(test_data), 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "w = logistic_regression()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(785,)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-e33ee31",
   "language": "python",
   "display_name": "PyCharm (ml_basics)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}