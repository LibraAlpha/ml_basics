{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 经验熵：$H(D) = Sum(|C_k|/|D| * log2(|C_k|/|D|)))$\n",
    "### 信息增益：$H(D) - H(D|A)$\n",
    "### 信息纯度：$Gini(D) = 1 - (sum(|C_k|/|D|))^2$\n",
    "- ID3：通过信息增益来进行分支，基础公式：\n",
    "- C4.5: 通过信息增益比\n",
    "- CART:Gini"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### $H(D|A) = Sum(D_i/D * H(D))$, $D_i$表示特征A取值为i的样本子集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, attribute=None, value=None, label=None):\n",
    "        self.attribute = attribute\n",
    "        self.value = value # 分裂属性的取值\n",
    "        self.label = label # 叶子节点的类标签\n",
    "        self.children = {} # 子节点的字典，格式为属性值：子节点"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DecisionTreeID3:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X,y)\n",
    "\n",
    "    def _build_tree(self, X, y):\n",
    "        attributes = list(range(X.shape[1])) # 获取所有属性的索引表\n",
    "\n",
    "        # 递归构建决策树\n",
    "        return self._recursive_build_tree(X, y, attributes)\n",
    "\n",
    "    def _recursive_build_tree(self, X, y, attributes):\n",
    "        node = Node()\n",
    "\n",
    "        # 所有的样本属于同一类别，直接设置叶子节点的类标签\n",
    "        if np.all(y == y[0]):\n",
    "            node.label = y[0]\n",
    "            return node\n",
    "\n",
    "        # 若属性列表为空，说明所有的属性都已用于构建决策树，无更多属性可供分裂，此时叶子节点的类标签为样本中最多的类别\n",
    "        if not attributes:\n",
    "            node.label = np.argmax(np.bincount(y))\n",
    "            return node\n",
    "\n",
    "        # 找到最优划分的特征，对其进行分裂\n",
    "        best_attr, best_value = self._choose_best_attr(X,y, attributes)\n",
    "\n",
    "        # 设置当前节点的分裂属性和取值\n",
    "        node.attribute = best_attr\n",
    "        node.value = best_value\n",
    "\n",
    "        attr_col = X[:, best_attr]\n",
    "        unique_values = np.unique(attr_col) # 根据特征的不同取值进行分裂\n",
    "\n",
    "        for value in unique_values:\n",
    "            # 选取分裂属性为best_attr,取值为value的样本\n",
    "            mask = attr_col == value\n",
    "            X_subset, y_subset = X[mask], y[mask]\n",
    "\n",
    "            # 从属性列表中移除best_attr，递归构建子树\n",
    "            attributes_subset = attributes.copy()\n",
    "            attributes_subset.remove(best_attr)\n",
    "\n",
    "            child_node = self._recursive_build_tree(X_subset, y_subset, attributes_subset)\n",
    "            node.children[value] = child_node\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _choose_best_attr(self, X, y, attributes):\n",
    "        best_attr = None\n",
    "        best_value = None\n",
    "\n",
    "        best_info_gain = -np.inf\n",
    "\n",
    "        # 计算初始信息熵\n",
    "        initial_entropy = self._calculate_entropy(y)\n",
    "\n",
    "        # 计算每个属性的信息增益，选择最大的信息增益\n",
    "        for attr in attributes:\n",
    "            attr_col = X[:, attr]\n",
    "            unique_values = np.unique(attr_col)\n",
    "            for value in unique_values:\n",
    "                # 根据属性和取值划分数据集\n",
    "                mask = attr_col == value\n",
    "                X_subset, y_subset = X[mask], y[mask]\n",
    "\n",
    "                # 计算划分后的加权信息熵\n",
    "                subset_entropy = self._calculate_entropy(y_subset)\n",
    "\n",
    "                info_gain = initial_entropy - subset_entropy\n",
    "\n",
    "                # 计算信息增益\n",
    "                if info_gain > best_info_gain:\n",
    "                    best_info_gain = best_info_gain\n",
    "                    best_attr = attr\n",
    "                    best_value = value\n",
    "\n",
    "        return best_attr, best_value\n",
    "\n",
    "    def _calculate_entropy(self, y):\n",
    "        # 计算给定标签的信息熵\n",
    "        class_cnt = np.bincount(y)\n",
    "        class_probs = class_cnt / len(y)\n",
    "        entropy = -np.sum(class_probs * np.log2(class_probs + 1e-8))\n",
    "        return entropy\n",
    "\n",
    "    def _traverse_tree(self, node:Node, instance):\n",
    "        # 遍历决策树，根据实例的属性值预测类别\n",
    "        if node.label is not None:\n",
    "            return node.label\n",
    "\n",
    "        attr_value = instance[node.attribute]\n",
    "        if attr_value in node.children:\n",
    "            chile_node = node.children[attr_value]\n",
    "            return self._traverse_tree(chile_node, instance)\n",
    "\n",
    "        return np.random.choice(np.unique(instance))\n",
    "\n",
    "    def predict(self, x):\n",
    "        # 如果当前节点是叶子节点，返回叶子节点的类别\n",
    "        node = self.root\n",
    "        while node.children:\n",
    "            attribute_value = x.item(node.attribute)\n",
    "            if attribute_value in node.children:\n",
    "                node = node.children[attribute_value]\n",
    "            else:\n",
    "                # 未找到对应的节点，返回根节点的类别\n",
    "                return node.label\n",
    "\n",
    "        return node.label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 模拟训练与测试数据\n",
    "X_train = np.array([[1, 'S'], [1, 'M'], [1, 'M'], [1, 'S'], [1, 'S'], [2, 'S'], [2, 'M'], [2, 'M'], [2, 'L'], [2, 'L'], [3, 'L'], [3, 'M'], [3, 'M'], [3, 'L'], [3, 'L']])\n",
    "y_train = np.array(['N', 'N', 'Y', 'Y', 'N', 'N', 'N', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'N'])\n",
    "\n",
    "X_test = np.array([[2, 'S'], [1, 'M'], [3, 'L'], [3, 'M'], [2, 'L']])\n",
    "y_test = np.array(['N', 'N', 'Y', 'Y', 'Y'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 对字符特征进行处理\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "X_train_encoded = X_train.copy()\n",
    "X_train_encoded[:,1] = encoder.fit_transform(X_train[:,1])\n",
    "\n",
    "y_train_encoded = encoder.fit_transform(y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_encoded = X_test.copy()\n",
    "X_test_encoded[:,1] = encoder.fit_transform(X_test[:,1])\n",
    "\n",
    "y_test_encoded = encoder.fit_transform(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 训练与测试验证\n",
    "dt = DecisionTreeID3()\n",
    "dt.fit(X_train_encoded, y_train_encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "err_cnt = 0\n",
    "for idx in range(len(X_test_encoded)):\n",
    "    y_pred = dt.predict(X_test_encoded[idx])\n",
    "    err_cnt += 1 if y_pred != y_test_encoded[idx] else 0\n",
    "\n",
    "acc = 1 - err_cnt / len(y_test_encoded)\n",
    "print(f'ACC is {acc:.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DecisionTreeID3:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X,y)\n",
    "\n",
    "    def _build_tree(self, X, y):\n",
    "        attributes = list(range(X.shape[1])) # 获取所有属性的索引表\n",
    "\n",
    "        # 递归构建决策树\n",
    "        return self._recursive_build_tree(X, y, attributes)\n",
    "\n",
    "    def _recursive_build_tree(self, X, y, attributes):\n",
    "        node = Node()\n",
    "\n",
    "        # 所有的样本属于同一类别，直接设置叶子节点的类标签\n",
    "        if np.all(y == y[0]):\n",
    "            node.label = y[0]\n",
    "            return node\n",
    "\n",
    "        # 若属性列表为空，说明所有的属性都已用于构建决策树，无更多属性可供分裂，此时叶子节点的类标签为样本中最多的类别\n",
    "        if not attributes:\n",
    "            node.label = np.argmax(np.bincount(y))\n",
    "            return node\n",
    "\n",
    "        # 找到最优划分的特征，对其进行分裂\n",
    "        best_attr, best_value = self._choose_best_attr(X,y, attributes)\n",
    "\n",
    "        # 设置当前节点的分裂属性和取值\n",
    "        node.attribute = best_attr\n",
    "        node.value = best_value\n",
    "\n",
    "        attr_col = X[:, best_attr]\n",
    "        unique_values = np.unique(attr_col) # 根据特征的不同取值进行分裂\n",
    "\n",
    "        for value in unique_values:\n",
    "            # 选取分裂属性为best_attr,取值为value的样本\n",
    "            mask = attr_col == value\n",
    "            X_subset, y_subset = X[mask], y[mask]\n",
    "\n",
    "            # 从属性列表中移除best_attr，递归构建子树\n",
    "            attributes_subset = attributes.copy()\n",
    "            attributes_subset.remove(best_attr)\n",
    "\n",
    "            child_node = self._recursive_build_tree(X_subset, y_subset, attributes_subset)\n",
    "            node.children[value] = child_node\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _choose_best_attr(self, X, y, attributes):\n",
    "        best_attr = None\n",
    "        best_value = None\n",
    "\n",
    "        best_info_gain = -np.inf\n",
    "\n",
    "        # 计算初始信息熵\n",
    "        initial_entropy = self._calculate_entropy(y)\n",
    "\n",
    "        # 与ID3不同，C4.5选择最大信息增益比\n",
    "        for attr in attributes:\n",
    "            attr_col = X[:, attr]\n",
    "            unique_values = np.unique(attr_col)\n",
    "            for value in unique_values:\n",
    "                # 根据属性和取值划分数据集\n",
    "                mask = attr_col == value\n",
    "                X_subset, y_subset = X[mask], y[mask]\n",
    "\n",
    "                # 计算划分后的加权信息熵\n",
    "                subset_entropy = self._calculate_entropy(y_subset)\n",
    "\n",
    "                info_gain = initial_entropy - subset_entropy\n",
    "\n",
    "                # 计算信息增益\n",
    "                if info_gain > best_info_gain:\n",
    "                    best_info_gain = best_info_gain\n",
    "                    best_attr = attr\n",
    "                    best_value = value\n",
    "\n",
    "        return best_attr, best_value\n",
    "\n",
    "    def _calculate_entropy(self, y):\n",
    "        # 计算给定标签的信息熵\n",
    "        class_cnt = np.bincount(y)\n",
    "        class_probs = class_cnt / len(y)\n",
    "        entropy = -np.sum(class_probs * np.log2(class_probs + 1e-8))\n",
    "        return entropy\n",
    "\n",
    "    def _traverse_tree(self, node:Node, instance):\n",
    "        # 遍历决策树，根据实例的属性值预测类别\n",
    "        if node.label is not None:\n",
    "            return node.label\n",
    "\n",
    "        attr_value = instance[node.attribute]\n",
    "        if attr_value in node.children:\n",
    "            chile_node = node.children[attr_value]\n",
    "            return self._traverse_tree(chile_node, instance)\n",
    "\n",
    "        return np.random.choice(np.unique(instance))\n",
    "\n",
    "    def predict(self, x):\n",
    "        # 如果当前节点是叶子节点，返回叶子节点的类别\n",
    "        node = self.root\n",
    "        while node.children:\n",
    "            attribute_value = x.item(node.attribute)\n",
    "            if attribute_value in node.children:\n",
    "                node = node.children[attribute_value]\n",
    "            else:\n",
    "                # 未找到对应的节点，返回根节点的类别\n",
    "                return node.label\n",
    "\n",
    "        return node.label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC is 1.00\n"
     ]
    }
   ],
   "source": [
    "err_cnt = 0\n",
    "for idx in range(len(X_test_encoded)):\n",
    "    y_pred = dt.predict(X_test_encoded[idx])\n",
    "    err_cnt += 1 if y_pred != y_test_encoded[idx] else 0\n",
    "\n",
    "acc = 1 - err_cnt / len(y_test_encoded)\n",
    "print(f\"ACC is {acc:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log2\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, attribute=None, value=None, label=None, split_point=None):\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "        self.label = label\n",
    "        self.split_point = split_point\n",
    "        self.children = {}\n",
    "\n",
    "class C45DecisionTree:\n",
    "    def __init__(self, min_samples_split=2):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y, attributes):\n",
    "        self.root = self._build_tree(X, y, attributes)\n",
    "\n",
    "    def _build_tree(self, X, y, attributes):\n",
    "        if len(set(y)) == 1:\n",
    "            # 所有样本属于同一类别，创建叶子节点\n",
    "            label = y[0]\n",
    "            return Node(label=label)\n",
    "\n",
    "        if len(attributes) == 0:\n",
    "            # 属性列表为空，创建叶子节点，类标签为样本中最多的类别\n",
    "            label = Counter(y).most_common(1)[0][0]\n",
    "            return Node(label=label)\n",
    "\n",
    "        # 选择最佳划分属性\n",
    "        best_attribute, split_point = self._choose_best_attribute(X, y, attributes)\n",
    "        node = Node(attribute=best_attribute, split_point=split_point)\n",
    "\n",
    "        if best_attribute is None:\n",
    "            # 没有合适的属性可用，创建叶子节点，类标签为样本中最多的类别\n",
    "            label = Counter(y).most_common(1)[0][0]\n",
    "            return Node(label=label)\n",
    "\n",
    "        # 根据最佳属性进行划分\n",
    "        attribute_values = X[:, attributes.index(best_attribute)]\n",
    "        if split_point is not None:\n",
    "            mask = attribute_values <= split_point\n",
    "            X_left, y_left = X[mask], y[mask]\n",
    "            X_right, y_right = X[~mask], y[~mask]\n",
    "            node.value = split_point\n",
    "        else:\n",
    "            attribute_values = X[:, attributes.index(best_attribute)]\n",
    "            unique_values = np.unique(attribute_values)\n",
    "            X_splits = []\n",
    "            y_splits = []\n",
    "            for value in unique_values:\n",
    "                mask = attribute_values == value\n",
    "                X_split, y_split = X[mask], y[mask]\n",
    "                X_splits.append(X_split)\n",
    "                y_splits.append(y_split)\n",
    "            node.value = unique_values\n",
    "\n",
    "        # 递归构建子节点\n",
    "        for i, value in enumerate(node.value):\n",
    "            if split_point is not None:\n",
    "                child_X, child_y = X_left, y_left if i == 0 else X_right, y_right\n",
    "            else:\n",
    "                child_X, child_y = X_splits[i], y_splits[i]\n",
    "            child_attributes = attributes.copy()\n",
    "            child_attributes.remove(best_attribute)\n",
    "            child_node = self._build_tree(child_X, child_y, child_attributes)\n",
    "            node.children[value] = child_node\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _choose_best_attribute(self, X, y, attributes):\n",
    "        num_samples = len(y)\n",
    "        entropy = self._calculate_entropy(y)\n",
    "\n",
    "        best_attribute = None\n",
    "        best_gain_ratio = 0.0\n",
    "        best_split_point = None\n",
    "\n",
    "        for attribute in attributes:\n",
    "            attribute_values = X[:, attributes.index(attribute)]\n",
    "\n",
    "            if all(isinstance(value, str) for value in attribute_values):\n",
    "                # 离散属性\n",
    "                information_gain = entropy - self._calculate_attribute_entropy(attribute_values, y)\n",
    "                gain_ratio = self._calculate_gain_ratio(information_gain, attribute_values, num_samples)\n",
    "\n",
    "                if gain_ratio > best_gain_ratio:\n",
    "                    best_gain_ratio = gain_ratio\n",
    "                    best_attribute = attribute\n",
    "                    best_split_point = None\n",
    "\n",
    "            else:\n",
    "                # 连续属性\n",
    "                sorted_indices = np.argsort(attribute_values)\n",
    "                sorted_attribute = attribute_values[sorted_indices]\n",
    "                sorted_labels = y[sorted_indices]\n",
    "\n",
    "                split_points = []\n",
    "                for i in range(len(sorted_attribute) - 1):\n",
    "                    if sorted_attribute[i] != sorted_attribute[i + 1]:\n",
    "                        split_points.append((sorted_attribute[i] + sorted_attribute[i + 1]) / 2)\n",
    "\n",
    "                for split_point in split_points:\n",
    "                    left_indices = attribute_values <= split_point\n",
    "                    right_indices = attribute_values > split_point\n",
    "\n",
    "                    left_labels = sorted_labels[left_indices]\n",
    "                    right_labels = sorted_labels[right_indices]\n",
    "\n",
    "                    left_entropy = self._calculate_entropy(left_labels)\n",
    "                    right_entropy = self._calculate_entropy(right_labels)\n",
    "                    information_gain = entropy - (len(left_labels) / num_samples) * left_entropy - (len(right_labels) / num_samples) * right_entropy\n",
    "\n",
    "                    gain_ratio = self._calculate_gain_ratio(information_gain, attribute_values, num_samples)\n",
    "\n",
    "                    if gain_ratio > best_gain_ratio:\n",
    "                        best_gain_ratio = gain_ratio\n",
    "                        best_attribute = attribute\n",
    "                        best_split_point = split_point\n",
    "\n",
    "        return best_attribute, best_split_point\n",
    "\n",
    "    def _calculate_entropy(self, labels):\n",
    "        class_counts = Counter(labels)\n",
    "        num_samples = len(labels)\n",
    "        entropy = 0.0\n",
    "\n",
    "        for count in class_counts.values():\n",
    "            probability = count / num_samples\n",
    "            entropy -= probability * log2(probability)\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    def _calculate_attribute_entropy(self, attribute, labels):\n",
    "        unique_values = np.unique(attribute)\n",
    "        num_samples = len(labels)\n",
    "        entropy = 0.0\n",
    "\n",
    "        for value in unique_values:\n",
    "            value_labels = labels[attribute == value]\n",
    "            value_count = len(value_labels)\n",
    "            value_entropy = self._calculate_entropy(value_labels)\n",
    "            entropy += (value_count / num_samples) * value_entropy\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    def _calculate_gain_ratio(self, information_gain, attribute, num_samples):\n",
    "        intrinsic_value = self._calculate_intrinsic_value(attribute, num_samples)\n",
    "        if intrinsic_value == 0:\n",
    "            return 0\n",
    "        return information_gain / intrinsic_value\n",
    "\n",
    "    def _calculate_intrinsic_value(self, attribute, num_samples):\n",
    "        value_counts = Counter(attribute)\n",
    "        intrinsic_value = 0.0\n",
    "\n",
    "        for count in value_counts.values():\n",
    "            probability = count / num_samples\n",
    "            intrinsic_value -= probability * log2(probability)\n",
    "\n",
    "        return intrinsic_value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-e33ee31",
   "language": "python",
   "display_name": "PyCharm (ml_basics)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}